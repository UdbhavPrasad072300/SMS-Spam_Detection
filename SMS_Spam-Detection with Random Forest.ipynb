{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Detection using NLP <br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Written by: Udbhav Prasad <br>\n",
    "Linkedin: https://www.linkedin.com/in/udbhav-prasad-1506b7192/ <br>\n",
    "HackerRank: https://www.hackerrank.com/uprasad1 <br>\n",
    "Github: https://github.com/UdbhavPrasad072300 <br>\n",
    "Computer Science Co-op - Ryerson University <br> <br> <br> <br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using nltk and sklearn to build a classifier to determine whether text is an SMS or SPAM(labeled \"ham\" for SMS and \"spam\" for SPAM)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We use the SMS Spam Collection Dataset from Kaggle: (https://www.kaggle.com/uciml/sms-spam-collection-dataset) <br>\n",
    "<br>\n",
    "To get a sense of the data we first see the first ten lines: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'spam.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5c9d6af6d30c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mislice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spam.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#reading csv file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mspam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spam.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from itertools import islice \n",
    "\n",
    "with open('spam.csv', 'r') as f: #reading csv file\n",
    "    reader = csv.reader(f)\n",
    "    spam = list(reader)\n",
    "\n",
    "spam = spam[1:]\n",
    "    \n",
    "for row in islice(spam, 10):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Cleaning up Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are empty values at the end of each rows which needs to be handled (last three columns): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncatedSpam = [row[0:2] for row in spam]\n",
    "for row in islice(truncatedSpam, 10):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We separate words with punctuation then filter the words to remove punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def remove_punctuation_and_lowercase(string1):\n",
    "    string1 = \"\".join([char for char in string1 if char not in string.punctuation])\n",
    "    return(string1.lower())\n",
    "\n",
    "for i in range(1, len(truncatedSpam)):\n",
    "    truncatedSpam[i][1] = truncatedSpam[i][1].split()\n",
    "    \n",
    "for row in range(1, len(truncatedSpam)):\n",
    "    for word in range(0, len(truncatedSpam[row][1])):\n",
    "        truncatedSpam[row][1][word] = remove_punctuation_and_lowercase(truncatedSpam[row][1][word])\n",
    "\n",
    "for i in islice(truncatedSpam, 10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "now we need to remove stopwords: <br>\n",
    "These are the stopwords in english which are in the nltk package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\") #stopwords in english from nltk package\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "we now remove the stopwords from our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(textList):\n",
    "    temp = [word for word in textList if word not in stopwords]\n",
    "    return temp\n",
    "\n",
    "for row in range(1, len(truncatedSpam)):\n",
    "    truncatedSpam[row][1] = remove_stopwords(truncatedSpam[row][1])\n",
    "\n",
    "for i in islice(truncatedSpam, 10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lemmatizing is important to get generalize words to their base meaning(singular, plural, past tense, present tense, future tense) <br>\n",
    "<br>\n",
    "Lemmatizing is slower than stemming but it much better for your classifier (we got all the time in the world right now,  as im probably going to upload this as a HTML file) <br>\n",
    "<br>\n",
    "We now lemmatize our words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "for row in range(1, len(truncatedSpam)):\n",
    "    for word in range(0, len(truncatedSpam[row][1])):\n",
    "        truncatedSpam[row][1][word] = wn.lemmatize(truncatedSpam[row][1][word])\n",
    "\n",
    "for i in islice(truncatedSpam, 10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Vectorizing Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Creating a sparse matrix for classifier (machine learning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#truncatedSpam = truncatedSpam[1:]\n",
    "\n",
    "tfidf_vect = TfidfVectorizer([i[1] for i in truncatedSpam])\n",
    "X_counts = tfidf_vect.fit_transform([i[1] for i in spam])\n",
    "print(X_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf vectorizer makes a vector based on the frequency of each possible word with a weighted value(weighting is based on values that seem to be a determining factor)\n",
    "\n",
    "There are other alternatives like countVectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Building the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import pandas as pd\n",
    "\n",
    "X_features = pd.DataFrame(X_counts.toarray()) #create dataframe for random forest\n",
    "#print(X_features.head(1))\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "k_fold1 = KFold(n_splits=5)\n",
    "\n",
    "k_fold2 = KFold(n_splits=10)\n",
    "\n",
    "print(cross_val_score(rf, X_features, [i[0] for i in truncatedSpam], cv=k_fold1, scoring='accuracy', n_jobs=-1))\n",
    "print('LINE BREAK'.center(100,'-'))\n",
    "print(cross_val_score(rf, X_features, [i[0] for i in truncatedSpam], cv=k_fold2, scoring='accuracy', n_jobs=-1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using k-fold cross validation for random forest classifier with k = 5 and k = 10, <br>\n",
    "Basically means that it splits the dataset into 10 and 5 pieces and evaluates on each piece with a classifier built from the remaining pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_features, [i[0] for i in truncatedSpam], test_size=0.2)\n",
    "\n",
    "temp = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)\n",
    "\n",
    "temp.fit(X_features, [i[0] for i in truncatedSpam]) #fit by data and their corresponding tags\n",
    "\n",
    "print(temp.predict_proba(X_features.head())) #head gives the first 5 rows, so prediction happens for the first five rows\n",
    "\n",
    "print(X_features.head())\n",
    "\n",
    "print([truncatedSpam[i][0] for i in range(0, 5)])\n",
    "\n",
    "#temp_model = temp.fit(X_train, Y_train)\n",
    "#sorted(zip(temp_model.feature_importances_, X_train.columns), reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The two columns above represent the probability of being either ham/spam\n",
    "In this case, the first one has probability of being ham by 95.54 percent (as well as the second, fourth and fifth).\n",
    "The third value has 75 percent probability of being spam and 25 percent being ham.\n",
    "Below the result i have shown the data and the tags for it (to verify that it is correct)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In conclusion, it has many flaws and shortcomings such as (there are most likely more): <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering (adding length of messages, amount of punctuation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing can be very much improvement with external data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many better methods of doing this, using different values or methods all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better/More data cleaning methods could be used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holdout sets are never an indicator of quality (well, maybe a little, but negligible)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
