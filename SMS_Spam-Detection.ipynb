{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Detection using NLP <br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Written by: Udbhav Prasad <br>\n",
    "Linkedin: https://www.linkedin.com/in/udbhav-prasad-1506b7192/ <br>\n",
    "HackerRank: https://www.hackerrank.com/uprasad1 <br>\n",
    "Github: https://github.com/UdbhavPrasad072300 <br>\n",
    "Computer Science Co-op - Ryerson University <br> <br> <br> <br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using nltk and sklearn to build a classifier to determine whether text is an SMS or SPAM(labeled \"ham\" for SMS and \"spam\" for SPAM)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We use the SMS Spam Collection Dataset from Kaggle: (https://www.kaggle.com/uciml/sms-spam-collection-dataset) <br>\n",
    "<br>\n",
    "To get a sense of the data we first see the first ten lines: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', '', '', '']\n",
      "['ham', 'Ok lar... Joking wif u oni...', '', '', '']\n",
      "['spam', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", '', '', '']\n",
      "['ham', 'U dun say so early hor... U c already then say...', '', '', '']\n",
      "['ham', \"Nah I don't think he goes to usf, he lives around here though\", '', '', '']\n",
      "['spam', \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\", '', '', '']\n",
      "['ham', 'Even my brother is not like to speak with me. They treat me like aids patent.', '', '', '']\n",
      "['ham', \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\", '', '', '']\n",
      "['spam', 'WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.', '', '', '']\n",
      "['spam', 'Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from itertools import islice \n",
    "\n",
    "with open('spam.csv', 'r') as f: #reading csv file\n",
    "    reader = csv.reader(f)\n",
    "    spam = list(reader)\n",
    "\n",
    "spam = spam[1:]\n",
    "    \n",
    "for row in islice(spam, 10):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Cleaning up Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are empty values at the end of each rows which needs to be handled (last three columns): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...']\n",
      "['ham', 'Ok lar... Joking wif u oni...']\n",
      "['spam', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]\n",
      "['ham', 'U dun say so early hor... U c already then say...']\n",
      "['ham', \"Nah I don't think he goes to usf, he lives around here though\"]\n",
      "['spam', \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\"]\n",
      "['ham', 'Even my brother is not like to speak with me. They treat me like aids patent.']\n",
      "['ham', \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\"]\n",
      "['spam', 'WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.']\n",
      "['spam', 'Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030']\n"
     ]
    }
   ],
   "source": [
    "truncatedSpam = [row[0:2] for row in spam]\n",
    "for row in islice(truncatedSpam, 10):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We separate words with punctuation then filter the words to remove punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...']\n",
      "['ham', ['ok', 'lar', 'joking', 'wif', 'u', 'oni']]\n",
      "['spam', ['free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005', 'text', 'fa', 'to', '87121', 'to', 'receive', 'entry', 'questionstd', 'txt', 'ratetcs', 'apply', '08452810075over18s']]\n",
      "['ham', ['u', 'dun', 'say', 'so', 'early', 'hor', 'u', 'c', 'already', 'then', 'say']]\n",
      "['ham', ['nah', 'i', 'dont', 'think', 'he', 'goes', 'to', 'usf', 'he', 'lives', 'around', 'here', 'though']]\n",
      "['spam', ['freemsg', 'hey', 'there', 'darling', 'its', 'been', '3', 'weeks', 'now', 'and', 'no', 'word', 'back', 'id', 'like', 'some', 'fun', 'you', 'up', 'for', 'it', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'to', 'send', 'å£150', 'to', 'rcv']]\n",
      "['ham', ['even', 'my', 'brother', 'is', 'not', 'like', 'to', 'speak', 'with', 'me', 'they', 'treat', 'me', 'like', 'aids', 'patent']]\n",
      "['ham', ['as', 'per', 'your', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'has', 'been', 'set', 'as', 'your', 'callertune', 'for', 'all', 'callers', 'press', '9', 'to', 'copy', 'your', 'friends', 'callertune']]\n",
      "['spam', ['winner', 'as', 'a', 'valued', 'network', 'customer', 'you', 'have', 'been', 'selected', 'to', 'receivea', 'å£900', 'prize', 'reward', 'to', 'claim', 'call', '09061701461', 'claim', 'code', 'kl341', 'valid', '12', 'hours', 'only']]\n",
      "['spam', ['had', 'your', 'mobile', '11', 'months', 'or', 'more', 'u', 'r', 'entitled', 'to', 'update', 'to', 'the', 'latest', 'colour', 'mobiles', 'with', 'camera', 'for', 'free', 'call', 'the', 'mobile', 'update', 'co', 'free', 'on', '08002986030']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def remove_punctuation_and_lowercase(string1):\n",
    "    string1 = \"\".join([char for char in string1 if char not in string.punctuation])\n",
    "    return(string1.lower())\n",
    "\n",
    "for i in range(1, len(truncatedSpam)):\n",
    "    truncatedSpam[i][1] = truncatedSpam[i][1].split()\n",
    "    \n",
    "for row in range(1, len(truncatedSpam)):\n",
    "    for word in range(0, len(truncatedSpam[row][1])):\n",
    "        truncatedSpam[row][1][word] = remove_punctuation_and_lowercase(truncatedSpam[row][1][word])\n",
    "\n",
    "for i in islice(truncatedSpam, 10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "now we need to remove stopwords: <br>\n",
    "These are the stopwords in english which are in the nltk package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\") #stopwords in english from nltk package\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "we now remove the stopwords from our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...']\n",
      "['ham', ['ok', 'lar', 'joking', 'wif', 'u', 'oni']]\n",
      "['spam', ['free', 'entry', '2', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005', 'text', 'fa', '87121', 'receive', 'entry', 'questionstd', 'txt', 'ratetcs', 'apply', '08452810075over18s']]\n",
      "['ham', ['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say']]\n",
      "['ham', ['nah', 'dont', 'think', 'goes', 'usf', 'lives', 'around', 'though']]\n",
      "['spam', ['freemsg', 'hey', 'darling', '3', 'weeks', 'word', 'back', 'id', 'like', 'fun', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'send', 'å£150', 'rcv']]\n",
      "['ham', ['even', 'brother', 'like', 'speak', 'treat', 'like', 'aids', 'patent']]\n",
      "['ham', ['per', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'set', 'callertune', 'callers', 'press', '9', 'copy', 'friends', 'callertune']]\n",
      "['spam', ['winner', 'valued', 'network', 'customer', 'selected', 'receivea', 'å£900', 'prize', 'reward', 'claim', 'call', '09061701461', 'claim', 'code', 'kl341', 'valid', '12', 'hours']]\n",
      "['spam', ['mobile', '11', 'months', 'u', 'r', 'entitled', 'update', 'latest', 'colour', 'mobiles', 'camera', 'free', 'call', 'mobile', 'update', 'co', 'free', '08002986030']]\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(textList):\n",
    "    temp = [word for word in textList if word not in stopwords]\n",
    "    return temp\n",
    "\n",
    "for row in range(1, len(truncatedSpam)):\n",
    "    truncatedSpam[row][1] = remove_stopwords(truncatedSpam[row][1])\n",
    "\n",
    "for i in islice(truncatedSpam, 10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lemmatizing is important to get generalize words to their base meaning(singular, plural, past tense, present tense, future tense) <br>\n",
    "<br>\n",
    "Lemmatizing is slower than stemming but it much better for your classifier (we got all the time in the world right now,  as im probably going to upload this as a HTML file) <br>\n",
    "<br>\n",
    "We now lemmatize our words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...']\n",
      "['ham', ['ok', 'lar', 'joking', 'wif', 'u', 'oni']]\n",
      "['spam', ['free', 'entry', '2', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005', 'text', 'fa', '87121', 'receive', 'entry', 'questionstd', 'txt', 'ratetcs', 'apply', '08452810075over18s']]\n",
      "['ham', ['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say']]\n",
      "['ham', ['nah', 'dont', 'think', 'go', 'usf', 'life', 'around', 'though']]\n",
      "['spam', ['freemsg', 'hey', 'darling', '3', 'week', 'word', 'back', 'id', 'like', 'fun', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'send', 'å£150', 'rcv']]\n",
      "['ham', ['even', 'brother', 'like', 'speak', 'treat', 'like', 'aid', 'patent']]\n",
      "['ham', ['per', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'set', 'callertune', 'caller', 'press', '9', 'copy', 'friend', 'callertune']]\n",
      "['spam', ['winner', 'valued', 'network', 'customer', 'selected', 'receivea', 'å£900', 'prize', 'reward', 'claim', 'call', '09061701461', 'claim', 'code', 'kl341', 'valid', '12', 'hour']]\n",
      "['spam', ['mobile', '11', 'month', 'u', 'r', 'entitled', 'update', 'latest', 'colour', 'mobile', 'camera', 'free', 'call', 'mobile', 'update', 'co', 'free', '08002986030']]\n"
     ]
    }
   ],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "for row in range(1, len(truncatedSpam)):\n",
    "    for word in range(0, len(truncatedSpam[row][1])):\n",
    "        truncatedSpam[row][1][word] = wn.lemmatize(truncatedSpam[row][1][word])\n",
    "\n",
    "for i in islice(truncatedSpam, 10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Vectorizing Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Creating a sparse matrix for classifier (machine learning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3550)\t0.1481298737377147\n",
      "  (0, 8030)\t0.22998520738984352\n",
      "  (0, 4350)\t0.3264252905795869\n",
      "  (0, 5920)\t0.2553151503985779\n",
      "  (0, 2327)\t0.25279391746019725\n",
      "  (0, 1303)\t0.24415547176756056\n",
      "  (0, 5537)\t0.15618023117358304\n",
      "  (0, 4087)\t0.10720385321563428\n",
      "  (0, 1751)\t0.2757654045621182\n",
      "  (0, 3634)\t0.1803175103691124\n",
      "  (0, 8489)\t0.22080132794235655\n",
      "  (0, 4476)\t0.2757654045621182\n",
      "  (0, 1749)\t0.3116082237740733\n",
      "  (0, 2048)\t0.2757654045621182\n",
      "  (0, 7645)\t0.15566431601878158\n",
      "  (0, 3594)\t0.15318864840197105\n",
      "  (0, 1069)\t0.3264252905795869\n",
      "  (0, 8267)\t0.18238655630689804\n",
      "  (1, 5504)\t0.27211951321382544\n",
      "  (1, 4512)\t0.4082988561907181\n",
      "  (1, 4318)\t0.5236458071582338\n",
      "  (1, 8392)\t0.4316010362639011\n",
      "  (1, 5533)\t0.5465881710238072\n",
      "  (2, 4087)\t0.07917128722158312\n",
      "  (2, 3358)\t0.11301399735581102\n",
      "  :\t:\n",
      "  (5570, 4218)\t0.12246610191126918\n",
      "  (5570, 8313)\t0.18723687600522523\n",
      "  (5570, 1084)\t0.11225268140936363\n",
      "  (5570, 4615)\t0.1596552981734164\n",
      "  (5570, 7039)\t0.18426763178390446\n",
      "  (5570, 3308)\t0.12172172618634511\n",
      "  (5570, 7627)\t0.10242646659763287\n",
      "  (5570, 1438)\t0.1429585509124154\n",
      "  (5570, 5334)\t0.21003730857873562\n",
      "  (5570, 2592)\t0.18458634504313887\n",
      "  (5570, 8065)\t0.2088086209859756\n",
      "  (5570, 1778)\t0.13664567516026058\n",
      "  (5570, 7049)\t0.205343868729306\n",
      "  (5570, 2892)\t0.2440099568063893\n",
      "  (5570, 3470)\t0.2752778321471702\n",
      "  (5570, 1786)\t0.28292057870729176\n",
      "  (5570, 3687)\t0.242516196245848\n",
      "  (5570, 4161)\t0.28292057870729176\n",
      "  (5570, 903)\t0.32476233976158125\n",
      "  (5570, 1546)\t0.340204888824892\n",
      "  (5571, 7756)\t0.14849350328973984\n",
      "  (5571, 5244)\t0.39009002726386227\n",
      "  (5571, 4225)\t0.5773238083586979\n",
      "  (5571, 7885)\t0.42752913176432156\n",
      "  (5571, 6505)\t0.5565029307246045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#truncatedSpam = truncatedSpam[1:]\n",
    "\n",
    "tfidf_vect = TfidfVectorizer([i[1] for i in truncatedSpam])\n",
    "X_counts = tfidf_vect.fit_transform([i[1] for i in spam])\n",
    "print(X_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf vectorizer makes a vector based on the frequency of each possible word with a weighted value(weighting is based on values that seem to be a determining factor)\n",
    "\n",
    "There are other alternatives like countVectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Building the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97399103 0.97488789 0.96947935 0.96499102 0.96678636]\n",
      "---------------------------------------------LINE BREAK---------------------------------------------\n",
      "[0.97670251 0.96236559 0.97127469 0.97666068 0.97127469 0.97307002\n",
      " 0.96768402 0.96947935 0.97127469 0.97486535]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import pandas as pd\n",
    "\n",
    "X_features = pd.DataFrame(X_counts.toarray()) #create dataframe for random forest\n",
    "#print(X_features.head(1))\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "k_fold1 = KFold(n_splits=5)\n",
    "\n",
    "k_fold2 = KFold(n_splits=10)\n",
    "\n",
    "print(cross_val_score(rf, X_features, [i[0] for i in truncatedSpam], cv=k_fold1, scoring='accuracy', n_jobs=-1))\n",
    "print('LINE BREAK'.center(100,'-'))\n",
    "print(cross_val_score(rf, X_features, [i[0] for i in truncatedSpam], cv=k_fold2, scoring='accuracy', n_jobs=-1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using k-fold cross validation for random forest classifier with k = 5 and k = 10, <br>\n",
    "Basically means that it splits the dataset into 10 and 5 pieces and evaluates on each piece with a classifier built from the remaining pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95979754 0.04020246]\n",
      " [0.95887734 0.04112266]\n",
      " [0.29674749 0.70325251]\n",
      " [0.95887734 0.04112266]\n",
      " [0.94026273 0.05973727]]\n",
      "   0     1     2     3     4     5     6     7     8     9     ...  8663  \\\n",
      "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "\n",
      "   8664  8665  8666  8667  8668  8669  8670  8671  8672  \n",
      "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 8673 columns]\n",
      "['ham', 'ham', 'spam', 'ham', 'ham']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_features, [i[0] for i in truncatedSpam], test_size=0.2)\n",
    "\n",
    "temp = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)\n",
    "\n",
    "temp.fit(X_features, [i[0] for i in truncatedSpam]) #fit by data and their corresponding tags\n",
    "\n",
    "print(temp.predict_proba(X_features.head())) #head gives the first 5 rows, so prediction happens for the first five rows\n",
    "\n",
    "print(X_features.head())\n",
    "\n",
    "print([truncatedSpam[i][0] for i in range(0, 5)])\n",
    "\n",
    "#temp_model = temp.fit(X_train, Y_train)\n",
    "#sorted(zip(temp_model.feature_importances_, X_train.columns), reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The two columns above represent the probability of being either ham/spam\n",
    "In this case, the first one has probability of being ham by 95.54 percent (as well as the second, fourth and fifth).\n",
    "The third value has 75 percent probability of being spam and 25 percent being ham.\n",
    "Below the result i have shown the data and the tags for it (to verify that it is correct)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In conclusion, it has many flaws and shortcomings such as (there are most likely more): <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering (adding length of messages, amount of punctuation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing can be very much improvement with external data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many better methods of doing this, using different values or methods all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better/More data cleaning methods could be used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holdout sets are never an indicator of quality (well, maybe a little, but negligible)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
